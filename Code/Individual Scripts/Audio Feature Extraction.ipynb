{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why we take only 12-13 MFCC coefficients in feature extraction? https://www.researchgate.net/post/Why_we_take_only_12-13_MFCC_coefficients_in_feature_extraction\n",
    "- MFCC - Significance of number of features: https://dsp.stackexchange.com/questions/28898/mfcc-significance-of-number-of-features\n",
    "- Even though higher order coefficients represent increasing levels of spectral details, depending on the sampling rate and estimation method, 12 to 20 cepstral coefficients are typically optimal for speech analysis. Selecting a large number of cepstral coefficients results in more complexity in the models. For example, if we intend to model a speech signal by a Gaussian mixture model (GMM), if a large number of cepstral coefficients is used, we typically need more data in order to accurately estimate the parameters of the GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding= UTF-8\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import soundfile as sf\n",
    "\n",
    "##Return audio features \n",
    "def feature_extraction(file_name):\n",
    "    #X, sample_rate = sf.read(file_name, dtype='float32')\n",
    "    X , sample_rate = librosa.load(file_name, sr=None) #Can also load file using librosa\n",
    "    if X.ndim > 1:\n",
    "        X = X[:,0]\n",
    "    X = X.T\n",
    "    \n",
    "    ## stFourier Transform\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "            \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=20).T, axis=0) #Returns N_mel coefs\n",
    "    rmse = np.mean(librosa.feature.rmse(y=X).T, axis=0) #RMS Energy for each Frame (Stanford's). Returns 1 value \n",
    "    spectral_flux = np.mean(librosa.onset.onset_strength(y=X, sr=sample_rate).T, axis=0) #Spectral Flux (Stanford's). Returns 1 Value\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=X).T, axis=0) #Returns 1 value\n",
    "    \n",
    "    #mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0) #Returns 128 values\n",
    "    #chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0) #Returns 12 values\n",
    "    #contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0) #Returns 7 values\n",
    "    #tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0) #tonal centroid features Returns 6 values\n",
    "    \n",
    "    ##Return computed audio features\n",
    "    return mfccs, rmse, spectral_flux, zcr\n",
    "    \n",
    "# Audio parsing: Function makes call for feature extraction and returns array with features and labels \n",
    "def parse_audio_files(parent_dir, sub_dirs, file_ext='*.mp3'): # Audio Format\n",
    "\n",
    "    n_mfccs = 20 # This variable is tunneable with each run\n",
    "    number_of_features = 3 + n_mfccs\n",
    "    #number_of_features = 154 + n_mfccs # 154 are the total values returned by rest of computed features\n",
    "    features, labels = np.empty((0,number_of_features)), np.empty(0)\n",
    "    \n",
    "    ##Extract features for each audio file\n",
    "    for label, sub_dir in enumerate(sub_dirs): ##The enumerate() function adds a counter to an iterable.\n",
    "        for file_name in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)): ##parent is data, sub_dirs are the classes\n",
    "            print \"Actual File Name: \", file_name\n",
    "            try:\n",
    "                mfccs, rmse, spectral_flux, zcr = feature_extraction(file_name)\n",
    "                #mfccs, zcr, mel, chroma, contrast, tonnetz = feature_extraction(file_name)\n",
    "            except Exception as e:\n",
    "                print(\"[Error] there was an error in feature extraction. %s\" % (e))\n",
    "                continue\n",
    "             \n",
    "            extracted_features = np.hstack([mfccs, rmse, spectral_flux, zcr])\n",
    "            #extracted_features = np.hstack([mfccs, zcr, mel, chroma, contrast, tonnetz]) #Stack arrays in sequence horizontally (column wise)\n",
    "            #print \"Total Extracted Features: \", len(extracted_features) #This helps us identify really how many features are being computed\n",
    "            features = np.vstack([features, extracted_features]) #Stack arrays in sequence vertically (row wise).\n",
    "            labels = np.append(labels, label)\n",
    "        print(\"Extrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'audio-data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-45ae7e52972d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Read audio classes directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maudio_subdirectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio-data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maudio_subdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Audio Subdirs: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_subdirectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'audio-data/'"
     ]
    }
   ],
   "source": [
    "#Read audio classes directories\n",
    "import os\n",
    "audio_subdirectories = os.listdir(\"audio-data/\") #Path to data folder.\n",
    "audio_subdirectories.sort()\n",
    "print 'Audio Subdirs: ', audio_subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get features and labels\n",
    "#This generates two numpy files. One npy file with feature vectors corresponding each audio file. The other with labels.\n",
    "\n",
    "# Parse Audio Files Function Call\n",
    "features, labels = parse_audio_files('audio-data', audio_subdirectories) #(parent dir,sub dirs)\n",
    "np.save('feat.npy', features) \n",
    "np.save('label.npy', labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 Conda2",
   "language": "python",
   "name": "anaconda2_py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
